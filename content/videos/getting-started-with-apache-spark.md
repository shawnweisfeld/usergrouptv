{
  "title": "Getting Started with Apache Spark",
  "type": "recording",
  "tags": [
    "Spark"
  ],
  "date": "2020-02-08T16:27:50",
  "draft": false,
  "slug": "getting-started-with-apache-spark",
  "abstract": "<p>As companies work to gain insight from ever-increasing amounts of data, data platform practitioners need tools which can scale along with the data. Early big data solutions in the Hadoop ecosystem assumed that data sizes overwhelmed available memory, emphasizing heavy disk usage to coordinate work between nodes. As the cost of memory decreases and the amount of memory available per server increases, we see a shift in the makeup of big data systems, emphasizing heavy memory usage instead of disk. Apache Spark, which focuses on memory-intensive operations, has taken advantage of this hardware shift to become the dominant solution for problems requiring distributed data. In this talk, we will take an introductory look at Apache Spark. We will review where it fits in the Hadoop ecosystem, cover how to get started and some of the basic functional programming concepts needed to understand Spark, and see examples of how we can use Spark to solve issues when analyzing large data sets.</p>",
  "description": "<p>As companies work to gain insight from ever-increasing amounts of data, data platform practitioners need tools which can scale along with the data. Early big data solutions in the Hadoop ecosystem assumed that data sizes overwhelmed available memory, emphasizing heavy disk usage to coordinate work between nodes. As the cost of memory decreases and the amount of memory available per server increases, we see a shift in the makeup of big data systems, emphasizing heavy memory usage instead of disk. Apache Spark, which focuses on memory-intensive operations, has taken advantage of this hardware shift to become the dominant solution for problems requiring distributed data. In this talk, we will take an introductory look at Apache Spark. We will review where it fits in the Hadoop ecosystem, cover how to get started and some of the basic functional programming concepts needed to understand Spark, and see examples of how we can use Spark to solve issues when analyzing large data sets.</p>",
  "images": [
    "https://i.vimeocdn.com/video/854621628_295x166.jpg"
  ],
  "vimeo": "390483472",
  "moreinfo": "https://www.sqlsaturday.com/939/Sessions/Details.aspx?sid=99470",
  "thumbnail": "https://i.vimeocdn.com/video/854621628_295x166.jpg",
  "mp4Video": "http://player.vimeo.com/external/390483472.hd.mp4?s=15ce8653bd588d60624e973c7115b926dc5eb171&profile_id=175&oauth2_token_id=20985841",
  "mp4VideoLow": "http://player.vimeo.com/external/390483472.sd.mp4?s=8581ddc007a1a342e2a3702ac9c622c01117f0f1&profile_id=165&oauth2_token_id=20985841",
  "recordingID": 332,
  "speakers": [
    {
      "name": "Kevin Feasel",
      "slug": "kevin-feasel",
      "bio": "<p>Kevin Feasel is a Data Platform MVP and Engineering Manager of the Predictive Analytics team at ChannelAdvisor, where he specializes in T-SQL and R development, fighting with Kafka, and pulling rabbits out of hats on demand. He is the lead contributor to Curated SQL (https://curatedsql.com), a contributing author to Tribal SQL (http://www.tribalsql.com), and one of the contributors behind We Speak Linux (https://wespeaklinux.com). A resident of Durham, North Carolina, he can be found cycling the trails along the triangle whenever the weather's nice enough.</p>",
      "count": 2
    }
  ],
  "ugtvtags": [
    {
      "name": "Spark",
      "slug": "spark",
      "count": 1
    }
  ],
  "meetups": [
    {
      "name": "Austin SQL Saturday",
      "slug": "austin-sql-saturday",
      "count": 27
    },
    {
      "name": "Capitol Area Central Texas Users of SQL Server",
      "slug": "capitol-area-central-texas-users-of-sql-server",
      "count": 73
    }
  ]
}